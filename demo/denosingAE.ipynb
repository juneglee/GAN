{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d70fd76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST, FashionMNIST\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f720a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57da5b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "batch_size = 128\n",
    "lr = 1e-3\n",
    "img_size = 28 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "848ba2b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ../data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b64620a2faac4cefa611a988c49d8065",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26421880 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ../data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ../data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "214549e5c77544988f30f98afc4cb33c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29515 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ../data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ../data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8daad409582419eb8aa1bee25aa349f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4422102 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ../data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ../data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c290addff7b43bd83a9afe8379c8341",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5148 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/FashionMNIST/raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    FashionMNIST('../data', download = True,transform=transform),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a292ca6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(28*28, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 12),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(12, 3),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(3, 12),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(12, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 28*28),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x) \n",
    "        decoded = self.decoder(encoded) \n",
    "        return encoded, decoded\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "053d6ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = Autoencoder().to(device)\n",
    "optimizer = torch.optim.Adam(\n",
    "    autoencoder.parameters(), lr=lr, weight_decay=1e-5\n",
    ")\n",
    "criterion = nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b5866f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(img):\n",
    "    noise = torch.randn(img.size()) * 0.2\n",
    "    noisy_img = img + noise\n",
    "    return noisy_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c5f894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch[1/10], loss:0.1319\n",
      "epoch[2/10], loss:0.0944\n",
      "epoch[3/10], loss:0.1023\n",
      "epoch[4/10], loss:0.0916\n",
      "epoch[5/10], loss:0.0918\n",
      "epoch[6/10], loss:0.0930\n",
      "epoch[7/10], loss:0.0783\n",
      "epoch[8/10], loss:0.0955\n",
      "epoch[9/10], loss:0.0981\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for data in dataloader:\n",
    "        img, _ = data\n",
    "        noise_x = add_noise(img)\n",
    "        noise_x = noise_x.view(-1, 28*28).to(device)\n",
    "        img = img.view(-1, 28*28).to(device)\n",
    "\n",
    "        encoded, decoded = autoencoder(noise_x)\n",
    "        loss = criterion(decoded, img)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print('epoch[{}/{}], loss:{:.4f}'\n",
    "          .format(epoch + 1, num_epochs, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af32835",
   "metadata": {},
   "outputs": [],
   "source": [
    "testSet = FashionMNIST('../data',\n",
    "                train = False,\n",
    "                download = False,\n",
    "                transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4363140b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = testSet.data[0].view(-1, 28*28)\n",
    "sample_data = sample_data.type(torch.FloatTensor)/255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463b7792",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_x = sample_data[0]\n",
    "noisy_x = add_noise(original_x).to(device)\n",
    "_, recovered_x = autoencoder(noisy_x) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d2d594",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, a = plt.subplots(1, 3, figsize=(15, 15))\n",
    "\n",
    "# 시각화를 위해 넘파이 행렬로 바꿔줍니다.\n",
    "original_img = np.reshape(original_x.to(\"cpu\").data.numpy(), (28, 28))\n",
    "noisy_img = np.reshape(noisy_x.to(\"cpu\").data.numpy(), (28, 28))\n",
    "recovered_img = np.reshape(recovered_x.to(\"cpu\").data.numpy(), (28, 28))\n",
    "\n",
    "# 원본 사진\n",
    "a[0].set_title('Original')\n",
    "a[0].imshow(original_img, cmap='gray')\n",
    "\n",
    "# 오염된 원본 사진\n",
    "a[1].set_title('Noisy')\n",
    "a[1].imshow(noisy_img, cmap='gray')\n",
    "\n",
    "# 복원된 사진\n",
    "a[2].set_title('Recovered')\n",
    "a[2].imshow(recovered_img, cmap='gray')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f52cb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
